{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In earlier notebook we discussed about the binary classification, here we are going to discuss about the 'regression', wchis consists of predicting continuous values instead of discrete labels.\n",
    "\n",
    "\n",
    "We are going to use the Boston housing Price dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import Dense, ReLU, Activation, Flatten, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, ZeroPadding2D\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea about the dataset, what are the feaures present in the dataset:-\n",
    "\n",
    "Per capita crime rate.\n",
    "Proportion of residential land zoned for lots over 25,000 square feet.\n",
    "Proportion of non-retail business acres per town.\n",
    "Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "Nitric oxides concentration (parts per 10 million).\n",
    "Average number of rooms per dwelling.\n",
    "Proportion of owner-occupied units built prior to 1940.\n",
    "Weighted distances to five Boston employment centres.\n",
    "Index of accessibility to radial highways.\n",
    "Full-value property-tax rate per $10,000.\n",
    "Pupil-teacher ratio by town.\n",
    "1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town.\n",
    "lower status of the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "[  1.23247   0.        8.14      0.        0.538     6.142    91.7\n",
      "   3.9769    4.      307.       21.      396.9      18.72   ]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[0]))\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of data\n",
    "\n",
    "1. using scikit learn feature to remove the mean and variance from the data. eature-wise normalization: for each feature in the input data (a column in the input data matrix), we will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a unit standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "print(scaler.fit(train_data))\n",
    "train_data = scaler.transform(train_data)\n",
    "\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
      "  0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
      "  0.8252202 ]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_layer,inp_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape = (train_data.shape[1],)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that we are not using sigmoid in the output layer, as we want it to be the continuous output.\n",
    "\n",
    "Note that we are compiling the network with the mse loss function: Mean Squared Error, the square of the difference between the predictions and the targets, a widely used loss function for regression problems.\n",
    "\n",
    "MAE of 0.5 on this problem would mean that our predictions are off by \\$500 on average\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using K-fold approach \n",
    "\n",
    "One way we saw in earlier notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
